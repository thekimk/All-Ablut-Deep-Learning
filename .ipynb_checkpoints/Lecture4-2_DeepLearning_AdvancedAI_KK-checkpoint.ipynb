{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **딥러닝 알고리즘의 발전(2010~)**\n",
    "\n",
    "[![Open in Colab](http://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thekimk/All-About-Deep-Learning/blob/main/Lecture4-1_DeepLearning_AdvancedAI_KK.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 위기극복의 계기\n",
    "\n",
    "- **NN의 두번째위기:** (1) `Vanishing Gradients`, (2) `Local Minimum`, (3) Low Learning Time, (4) Curse of Dimensionality\n",
    "\n",
    "> **\"RBM 이외에도, `Supervised Learning 문제에서의 아이디어로` 해법 개발 노력\"**\n",
    "\n",
    "---\n",
    "\n",
    "**1) Vanishing Gradients의 해결:** `활성화 함수 ReLU`\n",
    "\n",
    "<center><img src='Image/Expert/DL_ActivationFunction_Type.png' width='700'></center>\n",
    "\n",
    "- `Sigmoid`는 신경망이 깊어지면 여전히 `비용함수 최적화가 어려운 이슈` 발견\n",
    "- `다양한 미분가능한 비선형 활성화 함수` 필요성이 증가했고 `ReLU`가 Vanishing Gradient의 문제를 해결(Hinton et al. 2010)\n",
    "- 기존 활성화 함수는 양끝의 기울기가 0이 되는 이슈가 있었으나, ReLU는 `기울기가 0으로 감소하는 현상이 없고 일반적으로 학습성능도 향상`\n",
    "\n",
    "<center><img src='Image/Expert/Logistic_ReLU.png' width='400'></center>\n",
    "\n",
    "- 미세변화(미분) 중첩의 문제가 해결되면서 `선학습(Pre Training)도 불필요` (Bordes and Bengio. 2011)\n",
    "- 많은 종류의 비선형적 `활성화 함수가 지금도 개발중`\n",
    "\n",
    "<center><img src='Image/Expert/DL_ActivationFunction_Type_All.png' width='1100'>(https://miro.medium.com/max/814/1*F9-nc6ez5GOJ1mdB3TLlow.png)</center>\n",
    "\n",
    "---\n",
    "\n",
    "**2) Local Minimum의 해결:** `Global Minimum` $\\approx$ `Local Minimum`\n",
    "\n",
    "> - `고차원 데이터나 Non-convex 형태의 비용함수에서의 최적화`를 하더라도 Local Minimum들은 서로 유사할 것이며 Global Minimum과 큰 차이가 없을 것(Bengio et al. 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치 추정 최적화\n",
    "\n",
    "> **\"`비용함수가 가능한 낮은 가중치`를 찾아가는 과정\"**  \n",
    "> - **Prerequisite:** Derivatives, Partial Derivatives, Chain Rule\n",
    "\n",
    "---\n",
    "\n",
    "**1) 추정 프로세스:**\n",
    "\n",
    "<center><img src='Image/Expert/DNN_Process.PNG' width='700'>(https://www.inf.ufpr.br/todt/IAaplicada/CNN_Presentation.pdf)</center>\n",
    "\n",
    "> **(1) 네트워크 초기화:** `가중치의 초기값`이 필요하며 일반적으로 `무작위로 초기화` 됨   \n",
    ">\n",
    "> - 초기값이 모두 같으면 모든 노드들의 입력값에 동일 가중치가 반영되어 `같은 값이 출력되고` 역전파로 인해 `비용함수 변화량도 모두 동일`해지므로 다른값 필요   \n",
    ">\n",
    "> **(2) 순전파:** 초기화된 가중치들의 출력이 퍼셉트론을 거쳐 Output Layer에 도달\n",
    ">\n",
    "> - $\\hat{Y}_{init} = f(\\sum_{i}^{k} w_i x_i - \\theta)$  \n",
    ">\n",
    "> **(3) 비용함수 평가:**\n",
    ">\n",
    "> - **회귀문제:** $MSE = \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$    \n",
    ">\n",
    "> - **분류문제:** $Cross Entropy = \\sum_{i=1}^{k} \\left[ - \\color{red}{\\hat{Y}_{init} log (Pr(\\hat{Y}_{init}))} - \\color{blue}{(1-\\hat{Y}_{init}) log (1-Pr(\\hat{Y}_{init}))} \\right]$    \n",
    ">\n",
    "> **(4) 역전파:** `각 가중치 별 현재 비용함수에 미치는 영향` 계산 \n",
    ">\n",
    "> - $\\frac{\\delta E}{\\delta w} = \\frac{\\delta}{\\delta w} \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$  \n",
    ">\n",
    "> **(5) 가중치 업데이트:** 비용함수를 줄이는 방향으로 각 가중치 업데이트\n",
    ">\n",
    "> <center><img src='Image/Expert/DL_GD.PNG' width='400'></center>\n",
    ">\n",
    ">$$\n",
    "\\begin{aligned}\n",
    "W_1 &:= W_0 - \\alpha \\frac{\\partial}{\\partial w} \\left[ \\text{Cost Function} \\right] \\\\ &= W_0 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_0} \\\\\n",
    "W_2 &:= W_1 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_1} \\\\\n",
    "W_3 &:= W_2 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_2} \\\\\n",
    "& \\vdots \\\\\n",
    "W &:= W - \\alpha \\frac{\\partial C(W)}{\\partial W}\n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "> $$W_i^{new} = W_i^{old} - \\alpha \\Delta W_i^{old} = W_i^{old} - \\alpha \\frac{\\delta C}{\\delta w_i}$$  \n",
    ">\n",
    "> - **$\\alpha$:** `학습률(Learing Rate)`이 낮으면 추정이 느리고, 높으면 최적점을 벗어나 오차가 증가될 수 있음\n",
    "\n",
    "---\n",
    "\n",
    "**2) 예시:**\n",
    "\n",
    "> <center>$총비용(Y) = w_{정장}X_{정장} + w_{셔츠}X_{셔츠} + w_{타이}X_{타이}$</center>\n",
    "> <center><img src='Image/Expert/DL_Optimization_Example.png' width='400'></center>\n",
    ">\n",
    "> **(1+2) 네트워크 초기화 및 순전파:**  초기가중치($w^{initial}$)가 모두 150원이라면 `총비용은 1500원 출력`    \n",
    ">\n",
    "> **(3) 비용함수 평가:** \n",
    ">\n",
    "> - **회귀문제:** $MSE = \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2 = 423200$    \n",
    ">\n",
    "> **(4) 역전파:** $\\frac{\\delta E}{\\delta w} = \\frac{\\delta}{\\delta w} \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$  \n",
    ">\n",
    "> - $\\frac{\\partial MSE}{\\partial w_i} = \\frac{\\partial Y}{\\partial w_i} \\frac{d MSE}{dY} = X_i (\\hat{Y} - Y)$\n",
    ">\n",
    "> **(5) 가중치 업데이트:** $$W_i^{new} = W_i^{old} - \\alpha \\Delta W_i^{old} = W_i^{old} - \\alpha \\frac{\\delta C}{\\delta w_i}$$  \n",
    ">\n",
    "> $$\\rightarrow W_i^{new} = W_i^{initial} - \\alpha \\frac{\\delta MSE}{\\delta W_i} = W_i^{initial} - \\alpha X_i(\\hat{Y} - Y)$$ \n",
    ">\n",
    "> - **정장:** $$W_{정장}^{new} = W_{정장}^{initial} - \\alpha X_{정장} (\\hat{Y} - Y) \\\\ = 150 - (1/500)2(920) = 146.32$$\n",
    ">\n",
    "> - **셔츠:** $$W_{셔츠}^{new} = W_{셔츠}^{initial} - \\alpha X_{셔츠} (\\hat{Y} - Y) \\\\ = 150 - (1/500)5(920) = 140.8$$\n",
    ">\n",
    "> - **타이:** $$W_{타이}^{new} = W_{타이}^{initial} - \\alpha X_{타이} (\\hat{Y} - Y) \\\\ = 150 - (1/500)3(920) = 144.48$$\n",
    ">\n",
    "> **(2') 순전파:** `총비용은 1430.08원`(69.92 감소)\n",
    ">\n",
    "> **(3') 비용함수 평가:** $MSE = \\frac{1}{2}(580 - 1430.08)^2 = 361318$ (14.6% 감소)\n",
    ">\n",
    "> **(4') 역전파:** $\\frac{\\delta E}{\\delta w} = \\frac{\\delta}{\\delta w} \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$ \n",
    ">\n",
    "> **(5') 가중치 업데이트:** $$W_i^{new} = W_i^{old} - \\alpha \\Delta W_i^{old} = W_i^{old} - \\alpha \\frac{\\delta C}{\\delta w_i}$$  \n",
    ">\n",
    "> (...) **`비용함수가 더이상 변하지 않는` 최소값이 될때까지 반복하여 `목표값(580원)에 가까운 추정치 확보`**\n",
    "\n",
    "---\n",
    "\n",
    "**3) 결론:**\n",
    "\n",
    "<center><img src='Image/Expert/DL_MLP_Learning.PNG' width='600'></center>\n",
    "\n",
    "- **이슈:** 딥러닝은 추정해야할 가중치($W$)가 너무 많아 선형/로지시틱회귀분석에서의 `모든 가중치를 수학적 및 통계적으로 하나씩 규명 또는 추정하기 어려움`\n",
    "\n",
    "- **대응:** `비용함수를 최소`로 하는 위치의 가중치(W)를 추정하는 `최적화 알고리즘` 활용\n",
    "\n",
    "> - **Gradient Descent Algorithm:** 예시처럼 비용함수의 변화에 따라 `가중치 업데이트하는 대표적 알고리즘 `\n",
    ">\n",
    "> $$W_i^{new} = W_i^{old} - \\alpha \\Delta W_i^{old} = W_i^{old} - \\alpha \\frac{\\delta C}{\\delta w_i}$$  \n",
    ">\n",
    "> - **변화량크기:** 기울기 변화 크기로 `크면 가중치가 크게` 변경되고 `작으면 가중치가 작게` 변경\n",
    "> - **학습율(Learning Rate):** 이동속도로 `크면 가중치가 크게` 변경되고 `작으면 가중치가 작게` 변경\n",
    ">\n",
    "> <center><img src='Image/Expert/DL_Optimization_Flow.png' width='600'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**4) 최적화 알고리즘 종류 및 방향:**\n",
    "\n",
    "<center><img src='Image/Expert/DL_Optimization_Direction.png' width='700'></center>\n",
    "\n",
    "<!-- <center><img src='Image/Expert/DL_Optimization_Direction_KR.png' width='700'>(https://www.slideshare.net/yongho/ss-79607172)</center> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과적합 개선 아이디어\n",
    "\n",
    "> **\"Layer를 증가시킬수록 성능은 향상되지만 `연산이 기하급수적으로 늘어나 학습시간이 오래걸리고 과적합(Overfitting) 가능성 높아짐`\"**\n",
    "\n",
    "---\n",
    "\n",
    "**1) Regularization:** `비용함수를 개선`하여 과적합 낮춤\n",
    "\n",
    "> **\"`추정 가중치가 커지면` 활성함수를 통해 `기울기가 급변`하게되어 비용함수 최소화가 어렵고 `과적합 높아짐`\"**   \n",
    ">\n",
    "> **(0) Linear Regression:** `MSE`를 비용함수로 사용\n",
    ">\n",
    "> <center> $\\hat{w} = \\underset{w}{\\arg\\min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k w_i x_{ij}\\Bigr)^2\\Biggr]$ </center>\n",
    ">\n",
    "> **(1) L1 Panelty:** `LASSO Regression`에 사용한 비용함수 반영    \n",
    ">\n",
    "> \\begin{align*}\n",
    "\\hat{w} = \\underset{w}{\\arg\\min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k w_i x_{ij}\\Bigr)^2 + \\lambda \\displaystyle \\sum_{i=0}^k \\left|w_i \\right|\\Biggr] \\\\ where~\\lambda~is~hyper~parameter(given~by~human)\n",
    "\\end{align*}\n",
    ">\n",
    "> - `중요도가 낮은 변수`의 가중치는 0으로 출력하여 과적합 방지\n",
    "> - `변수선택 효과`가 있어 모델 복잡도를 효과적으로 제약\n",
    "> - 샘플 수보다 변수가 많더라도 변수선택 효과 때문에 `고차원의 데이터도 적용가능`\n",
    "> - `패널티의 정도`는 Hyperparameter로 사전 결정되며 교차검증이나 유사 방법으로 결정\n",
    "> - `모델에 제약`을 주며 정확도를 상승시킴   \n",
    ">\n",
    "> **(2) L2 Panelty:** `Ridge Regression`에 사용한 비용함수 반영   \n",
    ">\n",
    "> \\begin{align*}\n",
    "\\hat{w} = \\underset{w}{\\arg\\min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k w_i x_{ij}\\Bigr)^2 + \\lambda \\displaystyle \\sum_{i=0}^k w_i^2\\Biggr] \\\\ where~\\lambda~is~hyper~parameter(given~by~human)\n",
    "\\end{align*}\n",
    ">\n",
    "> - `모든 가중치`를 일률적으로 `작게 만드는 경향`\n",
    "> - `중요도 낮은 변수`라도 0이 아닌 가중치를 출력하므로 `일반화 및 변수비교 효과`\n",
    "> - 일반화 및 패널티 효과를 높이기 위해 `L1 보다 L2를 많이 사용하는 경향`\n",
    "\n",
    "<center><img src='Image/Expert/DL_CF_L1L2.png' width='600'></center>\n",
    "\n",
    "<center><img src='Image/Expert/DL_CF_L1L2_Compare.png' width='600'></center>\n",
    "<!-- (https://kevinthegrey.tistory.com/110) -->\n",
    "\n",
    "---\n",
    "\n",
    "**2) Drop Out:** 은닉층의 `일부 뉴런을 무작위 확률로 제외`하면서 학습 \n",
    "\n",
    "> **\"`모든 직원`이 함께 일하는 것 < `소그룹`의 결과를 통합하는 것 $\\rightarrow$ 더욱 효율적일수도\"**    \n",
    ">\n",
    "> **\"각 단계의 줄어든 뉴런은 약한 학습이지만 `약한 모델들이 합쳐져 강력한 예측력`\"**   \n",
    ">\n",
    "> - `Labeled 데이터의 부족`과 `Overfitting 문제 해결`을 위해 Drop Out 제안 (Hinton et al. 2012)\n",
    "> - 학습할 때마다 `일부 유닛만을 사용하고 이를 반복해서 합치는 방식`으로 `Ensemble과 유사`\n",
    "> - 빠진 뉴런들로 예측 하기에 `여러개의 국소적 독립적 내부패턴 학습가능`\n",
    "> - 네트워크가 `뉴런의 특정 가중치에 덜 민감해짐`\n",
    "> - `더욱 일반화에 기여가 가능`하고 훈련 데이터에만 과적합 가능성 적어짐\n",
    "> - 너무 낮은 비율은 효과가 적고 너무 높은 비율은 과소적합 하기에 `20~50% 권장`\n",
    "> - 일반적으로 `Learning Rate(LR, 10->100)과 Momentum(0.9 or 0.99)을 높여 사용`\n",
    "> - LR을 높여 가중치의 크기를 줄이면 Ridge와 유사하게 `과적합이 줄어 높은 성능`\n",
    "\n",
    "<center><img src='Image/Expert/DL_DropOut.png' width='500'></center>\n",
    "<!-- (https://t1.daumcdn.net/cfile/tistory/99324B335D383CBD1B) -->\n",
    "\n",
    "> - 랜덤한 뉴런을 사용하는 것 대신, `연결선을 랜덤하게 사용하는 DropConnect 방법`도 존재 (Wan et al. 2013)\n",
    "\n",
    "<center><img src='Image/Expert/DL_Dropout_Dropconnect.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**3) Early Stopping:** Test 성능이 빠르게 높아진 경우 `계속 학습하지 않고 일찍 종료`하는 방법   \n",
    "\n",
    "> **\"데이터가 Train/Validation/Test로 구분되어 있을때, `Validation/Test의 비용함수가 낮으면 멈추므로 Train을 계속 학습하는 과적합 가능성 낮춤`\"**   \n",
    "\n",
    "<center><img src='Image/Expert/DL_Overfitting_Epoch.png' width='600'></center>\n",
    "\n",
    "<center><img src='Image/Expert/DL_Overfitting_EarlyStopping.png' width='600'></center>\n",
    "<!-- (https://kevinthegrey.tistory.com/110) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 최적화\n",
    "\n",
    "> **\"최종적으로 `미래 데이터 예측에 적합한 최적 가이드를 하이퍼파라미터로 표현`\"**\n",
    "\n",
    "<center><img src='Image/Expert/Hyperparameter_Tuning.png' width='900'>(Hyperparameter tuning for big data using Bayesian optimisation)</center>\n",
    "\n",
    "---\n",
    "\n",
    "**0) 에러분석:** `에러를 수동으로 분석`하여 `모델링에 도움될 패턴을 찾아내어 반영`\n",
    "\n",
    "**1) 학습성능변화:** 하이퍼파라미터의 변화에 따른 `성능변화를 추적하여 최적 하이퍼파라미터 선택`    \n",
    "\n",
    "> - `데이터 분리 비율`\n",
    "> - `가중치 초기값`\n",
    "> - `은닉층의 수`\n",
    "> - `Regularized 비중`\n",
    "> - `DropOut 비율`\n",
    "> - `Cost Function 선정`\n",
    "> - `Learning Rate 크기`\n",
    "> - `Early Stopping 고려기간`\n",
    "> - `Batch 크기`\n",
    "> - `Epoch 횟수`\n",
    "\n",
    "---\n",
    "\n",
    "- **예시:**\n",
    "\n",
    "<center><img src='Image/Expert/DL_hyperparameter_epoch.png' width='600'></center>\n",
    "\n",
    "<center><img src='Image/Expert/DL_hyperparameter_search_simple.jpg' width='800'></center>\n",
    "\n",
    "<center><img src='Image/Expert/DL_hyperparameter_table_simple.png' width='350'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 발전 요약\n",
    "\n",
    "> **1) 1950년대 `퍼셉트론(Perceptron)`에서 시작된 인공신경망 연구는 1980년대 `오류의 역전파알고리즘(Error Backpropagation Algorithm)`으로 `다층퍼셉트론(Multilayer perceptron)`을 학습할 수 있게 되면서 발전을 이루었다.**\n",
    ">\n",
    "> **2) 하지만 `Gradient Vanishing, Labeled 데이터의 부족, Overfitting, Local Minimum 등`이 잘 해결되지 못해 2000년대 초까지 인공신경망 연구는 잠시 지지부진 하였고,** \n",
    ">\n",
    "> **3) 2006년부터 `볼츠만머신을 이용한 Unsupervised Learning`인 Restricted Boltzmann Machine(RBM), Deep Belief Network(DBN), Deep Boltzmann Machine(DBM), Convolutional Deep Belief Network 등이 개발되면서 Unlabeled data를 이용하여 `Pre-training을 수행할 수 있게 되어 위에 언급된 다층퍼셉트론의 한계점이 극복`되었다.**\n",
    ">\n",
    "> **4) 2010년부터 `빅데이터를 적극적으로 이용`함으로서 수많은 Labeled 데이터를 사용할 수 있게 되었고,**\n",
    ">\n",
    "> **5) `Rectified Linear Unit (ReLU), DropOut, DropConnect 등의 발견`으로 Vanishing Gradient문제와 Overfitting 이슈를 해결하여 Supervised Learning이 가능하게 되었으며, `Local Minimum 문제도 High Dimension Non-convex Optimization에서 얼마나 Global Minimum과 차이가 나는지 연구되고 있으며 큰 차이가 없다`라는 연구도 있다.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **다양한 딥러닝 알고리즘의 등장**\n",
    "\n",
    "> **\"인공지능 알고리즘은 지난 60여 년 동안 `두 번의 침체기와 두 번의 전성기`를 겪었고\n",
    "현재 인공지능은 `세 번째 전성기 대두`\"**\n",
    ">\n",
    "> - **1차 전성기(1950년대 후반~1960년대 초):** 추론과 탐색 기법 중심이었으나 `간단한 문제를 해결`하는 것 외에 `뚜렷한 가능성을 제시하지 못하고` 곧바로 1차 침체기를 경험함\n",
    ">\n",
    "> - **2차 전성기(1980년대 후반~1990년대 초):** `특정 분야`에서 `정해진 규칙에 따라 전문가시스템`을 구축하는 방식이었으나 `확장성 측면에서 한계`를 보이며 2차 침체기를 맞이함\n",
    ">\n",
    "> - `2012년 ImageNet Challenge 대회`에서 딥러닝이 압도적 성능으로 우승한 이후 `DNN/CNN/RNN 등 알고리즘이 비약적으로 발전`\n",
    ">\n",
    "> - 신경망의 `고전적 문제들은 대부분 해결`되고 `빅데이터`의 폭발적인 증가와 `하드웨어 기술의 발전`으로 산업에 본격적으로 활용\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**0) MLP(Multi-Layered Perceptron):** 데이터의 `패턴 또는 지식`을 추론하는 방법\n",
    "\n",
    "- 은닉층의 수가 증가하면 더욱 어려운 문제를 풀수 있는데, 통상 `은닉층을 최소 2개이상 가진 알고리즘`을 `딥러닝(Deep Learning)` 알고리즘 이라고 함\n",
    "\n",
    "<center><img src='Image/Expert/DL_MLP_Custom.PNG' width='500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Convolutional Neural Network(CNN) 발전\n",
    "\n",
    "<center><img src='Image/Expert/CNN_Example.webp' width='900'></center>\n",
    "\n",
    "- `기존`의 방식은 `데이터에서 지식을 추출`해 학습이 이루어졌지만, CNN은 `데이터의 특징을 추출하여 특징들의 패턴을 파악`하는 구조\n",
    "\n",
    "- `이미지처리`를 위해 특수 고안된 구조로 `데이터의 패턴을 추출하고(Convolution) 차원을 줄여 일반적인 패턴으로 정교화(Pooling)`하는 층으로 구성\n",
    "\n",
    "> - `Lenet5 (1998)`\n",
    "> - `Alexnet (2012)`\n",
    "> - `ZFNet (2013)`\n",
    "> - `VGGNet (2014)`\n",
    "> - `GoogLeNet (2014)`\n",
    "> - `ResNet (2015)`\n",
    "\n",
    "---\n",
    "\n",
    "**1) Lenet5 (1998)**\n",
    "\n",
    "> <center><img src='Image/Expert/CNN_Lenet5.PNG' width='800'>(A Shallow Convolutional Neural Network for Accurate Handwritten Digits Classification, 2017)</center>\n",
    ">\n",
    "> - Image Classification에 거의 보편적으로 사용되는 `Convolutional Neural Network(CNN)을 최초로 제안한 논문`인 Yann LeCun의 LeNet-5\n",
    "> - MLP가 가지는 한계점인 `입력 pixel수가 많아지면 parameter가 기하급수적으로 증가`하는 문제, `국소 이미지의 왜곡 문제 등`을 지적하며, 이러한 문제를 해결할 수 있는 `Convolutional Neural Network 구조를 처음 제안`\n",
    "> - `입력층을 1차원적 관점에서 2차원으로 확장`하였고, `추정 가중치를 공유`하기 때문에 입력 데이터수가 증가해도 `파라미터 수가 변하지 않아 학습속도가 빠르고 일반화 능력 우수`\n",
    "> - `그레이스케일 입력 이미지`로 `디지털화된 수표(수표)의 손으로 쓴 숫자를 인식`하기 위해 여러 `은행에서 적용`\n",
    "\n",
    "---\n",
    "\n",
    "**2) Alexnet (2012)**\n",
    "\n",
    "> <center><img src='Image/Expert/CNN_AlexNet.PNG' width='700'>(ImageNet Classification with Deep Convolutional)</center>\n",
    ">\n",
    "> - Classification 성능을 겨루는 대회인 `ILSVRC 대회`가 2010년부터 매년 열렸는데, SuperVision 이라는 이름의 팀이 `2012년 압도적인 성능으로 우승`하게 되고 이때 사용한 모델이 AlexNet\n",
    "> - AlexNet은 Alex Krizhevsky, Geoffrey Hinton 및 Ilya Sutskever 교수진이 개발한 `딥러닝의 혁명을 일으킨 CNN 모델`\n",
    "> - `활성함수는 ReLU + GPU를 사용한 Convolution 연산`\n",
    "> - Lenet5에 비해 학습시간 단축 및 `Dropout 및 PCA를 이용한 Data Augmentation으로 과적합 방지`\n",
    "\n",
    "---\n",
    "\n",
    "**3) ZFNet (2013)**\n",
    "\n",
    "> <center><img src='Image/Expert/CNN_ZFNet.PNG' width='800'>(Visualizing and Understanding Convolutional Networks)</center>\n",
    ">\n",
    "> - `ILSVRC 2013 대회에서 우승`한 Clarifai 팀의 Zeiler와 Fergus의 이름을 따서 지은 ZFNet\n",
    "> - `AlexNet을 기반`으로 첫 Conv layer의 filter size를 11에서 7로, stride를 4에서 2로 바꾸고, 그 뒤의 Conv layer들의 filter 개수를 키워주는 등(Conv3,4,5: 384, 384, 256 –> 512, 1024, 512) 약간의 튜닝\n",
    "> - 학습이 진행됨에 따라 `Feature Map을 시각화`하는 방법과, 모델이 `어느 영역을 보고 예측을 하는지 관찰`하기 위한 Occlusion 기반의 Attribution 기법 등 `시각화 측면에 집중`\n",
    "\n",
    "---\n",
    "\n",
    "**4) VGGNet (2014)**\n",
    "\n",
    "> <center><img src='Image/Expert/CNN_VGGNet_Comparison.png' width='600'>(http://cs231n.stanford.edu/)</center>\n",
    ">\n",
    "> - `옥스포드 연구진에서 2014년` 발표한 VGG로 `ILSVRC 2014 대회에서 2위`의 성적\n",
    "> - 이전 방식들과는 다르게 `비교적 작은 크기인 3x3 convolution filter를 깊게 쌓는다`는 것이 VGG의 핵심\n",
    "> - 현재 `이미지 패턴 추출 커뮤니티에서 가장 선호되는` 방식 중 하나\n",
    "> - 오류율이 7.3%로 줄어들어 `정상적인 사람의 이미지 분류성능과 매우 근접한 수준`\n",
    "\n",
    "---\n",
    "\n",
    "**5) GoogLeNet (2014)**\n",
    "\n",
    "> <center><img src='Image/Expert/CNN_GoogLeNet.png' width='800'>(Design and Development of Diabetes Management System Using Machine Learning)</center>\n",
    ">\n",
    "> - `ILSVRC 2014 대회에서 1위`를 하였으며 `Inception Architecture라는 예명`\n",
    "> - GoogLeNet을 기점으로 `거대 기업들이 뛰어들었다는 점`이 주목할만 하고 `Google과 LeNet을 합쳐서 작명`\n",
    "> - `Global Average Pooling(GAP)`를 사용하여 총 1024개의 노드를 만든 뒤 class 개수(ImageNet=1000)의 output을 출력하도록 하나의 Fully-Connected layer만 사용하여 AlexNet, ZFNet, VGG 등에 비해 `훨씬 적은 수의 파라미터`\n",
    "> - 22층의 `CNN으로 입센셥 모듈이라는 블록을 반복 사용`하는 모델\n",
    "> - 인셉션 모듈은 `CNN의 최적 희소구조를 찾아내고 이를 사용가능한 패턴으로 근사`\n",
    "\n",
    "---\n",
    "\n",
    "**6) ResNet (2015)**\n",
    "\n",
    "> <center><img src='Image/Expert/CNN_ResNet.PNG' width='1000'>(Deep Residual Learning for Image Recognition)</center>\n",
    ">\n",
    "> - `Microsoft Research`에서 제안한 구조이며 `ILSVRC 2015 대회에서 1위`를 차지하며 `최초로 사람의 분류 성능을 뛰어넘은 모델`로 평가\n",
    "> - Layer의 개수에 따라 `ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152 등 5가지 버전`으로 나타낼 수 있으며, ILSVRC 2015 대회에선 ResNet-152로 1위를 차지\n",
    "> - `152개의 레이어`로 훈련할 수 있고 `인간 수준의 성능을 능가하는 3.57% 오류율`\n",
    "> - 층이 많아지다 보니 연산량도 많아지게 되는데, Inception module에서 보았던 `Bottleneck 구조를 차용하여 Bottleneck Residual Block 을 중첩하여 사용`하는 점이 특징\n",
    "> - 이후 모델은 `ResNeXt, DenseNet, MobileNets 등`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network(RNN) 발전\n",
    "\n",
    "> **\"일상 생활에서의 `기계변역, 음성인식, 텍스트분류, DNA 시퀀싱 분석 등` Sequential Modeling 이슈는 항상 존재하며 `대부분 지도학습`에 속하고, `입력과 출력의 크기는 가변적`\"**\n",
    ">\n",
    "> - 1986년에 개발되어 `반복적이고 순차적인 데이터(Sequential data)학습에 특화`된 인공신경망의 한 종류로써 내부 뉴런들이 순환구조로 연결되어 `과거 정보를 기억하는 특징`이 있고 `입출력의 크기를 가변적 조절 가능`\n",
    "\n",
    "<center><img src='Image/Expert/RNN_Example.webp' width='400'>(https://medium.datadriveninvestor.com/recurrent-neural-network-with-keras-b5b5f6fe5187)</center>\n",
    "\n",
    "- 순환구조를 이용하여 `과거의 학습을 Weight를 통해 현재 학습에 반영`\n",
    "- 기존의 지속적이고 `반복적이며 순차적인 데이터학습의 한계를 해결`한 알고리즘\n",
    "- `현재의 학습과 과거의 학습의 연결`을 가능하게 하고 `시간에 종속`된다는 특징\n",
    "- 훨씬 많은 정보와 공간이 있는 이미지와 달리, `RNN 아키텍처의 수는그렇게 다채롭지 않음` \n",
    "- RNN 발전의 핵심은 `과거의 중요한 정보를 최대한 많이 기억하려는 방향`\n",
    "- CNN으로도 적절한 데이터 처리로 가능할 순 있지만 `쉽지 않으며 굳이 성격이 다른 모델을 고려할 필요 없음`\n",
    "\n",
    "> - `RNN (1986)`\n",
    "> - `LSTM (1997)`\n",
    "> - `GRU (2014)`\n",
    "> - `Sequence-to-Sequence (Seq2Seq, 2014)`\n",
    "> - `Attention (2015)`\n",
    "> - `Transformer (2017)`\n",
    "\n",
    "---\n",
    "\n",
    "**0) RNN의 다양한 입출력 처리:** `일반적 신경망`은 `다양한 크기의 입출력 처리 어려움`\n",
    "\n",
    "- **장점:** 크게 4가지 유형으로 분류되며 `무엇을 입력/출력하는지에 따라 유연하게 활용` 가능\n",
    "\n",
    "<center><img src='Image/Expert/DL_RNN_Type.PNG' width='700'></center>\n",
    "\n",
    "> - **One-to-One:** Vanilla Neural Networks\n",
    "> - **Many-to-One:** Classification, Time Series Analysis, Sentiment Analysis, Spam Detection\n",
    "> - **One-to-Many:** Image Captioning, Target Explanation\n",
    "> - **Many-to-Many:** Time Series Analysis, Machine Translation, Prediction of Next Word, Video Classification (`return_sequences = True`)\n",
    "\n",
    "- **단점:** `과거를 기억하는 어려움` 및 기울기가 0이 되거나 폭발하여 `메모리 부족`\n",
    "\n",
    "---\n",
    "\n",
    "**1) LSTM (1997)**\n",
    "\n",
    "> <center><img src='Image/Expert/LSTM_Architecture.PNG' width='700'>(https://blog.mlreview.com/understanding-lstm-and-its-diagrams-37e2f46f1714)</center>\n",
    ">\n",
    "> - `기존 RNN`이 소수의 매개변수가 너무 많은 과거 정보를 처리 및 기억하여 `쉽게 과부화 되는 이슈를 해결`한 가장 인기있는 아키텍처\n",
    "> - 과거의 정보 중`잊어도 되는 정보 + 강조되어야 하는 정보 + 출력할 것`으로 선택적 처리 반영\n",
    "> - 서로 다른 목적의 `3가지의 논리층`이 `장단기 정보의 기억 성능`을 크게 향상\n",
    "\n",
    "---\n",
    "\n",
    "**2) GRU (2014)**\n",
    "\n",
    "> <center><img src='Image/Expert/DL_LSTM_GRU.png' width='700'>(https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)</center>\n",
    ">\n",
    "> - `LSTM보다 간략화한 구조`로 한국인 뉴욕대 조경현 교수님이 개발 (Cho et al. 2014)\n",
    "> - LSTM의 개념은 유지한 채 `매개변수를 줄여 계산 시간을 크게 줄임`\n",
    "> - LSTM과 하이퍼파라미터 설정에 따라 성능이 달라지겠지만, `데이터셋이 적거나 모델의 반복 시도가 많은 경우` 특히 적합할 수 있음\n",
    "\n",
    "---\n",
    "\n",
    "**3) Sequence-to-Sequence (Seq2Seq, 2014)**\n",
    "\n",
    "> <center><img src='Image/Expert/Seq2Seq_Arch1.PNG' width='800'>(https://wikidocs.net/24996)</center>\n",
    ">\n",
    "> <center><img src='Image/Expert/Seq2Seq_Arch2.PNG' width='800'>(https://jeddy92.github.io/)</center>\n",
    ">\n",
    "> - Encoder와 Decoder라는 개념을 사용한 `2개의 RNN을 연결(Many-to-One + One-to-Many)`하여 `하나의 시계열 데이터를 다른 시계열로 변환`\n",
    "> - 구조적으로 `고정 크기의 벡터에 정보를 압축`하다보니 `정보손실 발생`\n",
    "> - 여전히 `병렬처리가 어렵고 많은 계산 복잡도`\n",
    "> - `입력데이터가 길어지면 성능이 크게 하락`되는 현상 존재\n",
    "> - 이후 개발된 `Attention & Transformer`가 등장하기 전까지 `딥러닝 기반 번역의 돌파구 역할`\n",
    "---\n",
    "\n",
    "**4) Attention (2015)**\n",
    "\n",
    "> <center><img src='Image/Expert/Attention_Architecture.PPM' width='600'>(Neural Abstractive Text Summarization with Sequence-to-Sequence Models)</center>\n",
    ">\n",
    "> - Seq2Seq의 Encoder에서 `일정 크기로 정보를 압축하다 정보손실이 발생하는 문제 보완`\n",
    "> - Encoder의 압축정보 이외에 `Encoder의 전체 입력 데이터를 구가 검토하여 중요도 반영`\n",
    "> - 해당 시점에 `예측해야 할 데이터와 관련있는 입력데이터를 조금 더 집중`해서 반영\n",
    "> - 즉, 반영되지 않은 은닉층의 정보 중 `출력층과 연관성이 높은데이터에 가중치` \n",
    "> - Decoder가 Encoder에 입력되는 모든 단어의 정보를 활용할 수 있기 때문에 `장기 의존성 문제를 해결`\n",
    "> - 여전히 `병렬처리가 어려운 근본적인 단점과 많은 계산 복잡도`\n",
    "\n",
    "---\n",
    "\n",
    "**5) Transformer (2017)**\n",
    "\n",
    "> <center><img src='Image/Expert/Transformer_Architecture.PNG' width='900'>([CS224n] Lecture 14: Transformers and Self-Attention for Generative Models)</center>\n",
    ">\n",
    "> - `RNN을 기반으로 하지 않는` 또다른 Encoder-Decoder 구조\n",
    "> - 인간의 뇌에서 정보를 `잠재공간(Latent Space)`에 저장 후 `다양한 분야에 재활용` 응용\n",
    "> - Attention 알고리즘의 단점인 `계산 복잡도를 줄임`\n",
    "> - 입력 데이터의 관계정보를 `병렬처리를 통한 Self-attention`을 사용하여 미리 계산 및 저장하여 `미래 데이터를 효율적으로 예측`\n",
    "> - Self-attention을 동시에 여러개로 수행하고 `Step마다 다른 Attention 결과`가 제시되어 `앙상블과 유사한 효과`\n",
    "> - 데이터를 학습할 때 `CNN과 RNN 보다 유의하게 더욱 빠르며 입출력 데이터의 거리에 관계없이 동작가능`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "428px",
    "left": "23px",
    "top": "110.229px",
    "width": "388.5px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
