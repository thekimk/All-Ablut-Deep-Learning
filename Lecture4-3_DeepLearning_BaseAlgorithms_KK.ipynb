{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **기초 알고리즘과 신경망 알고리즘 비교**\n",
    "\n",
    "[![Open in Colab](http://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thekimk/All-About-Deep-Learning/blob/main/Lecture4-2_DeepLearning_BaseAlgorithms_KK.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통계추론에서 기계학습/딥러닝학습으로\n",
    "\n",
    "> **\"데이터 과학은 크게 `2가지 관점`으로 발전\"**\n",
    "> - **통계학(Inferential Statistics):**\n",
    ">> - 데이터 과학의 근간\n",
    ">> - `통계학 기반의 다양한 기능들`은 딥러닝, 패턴인식, 기계학습 등에서 사용중\n",
    "> - **컴퓨터공학(Computer Science):**\n",
    ">> - `부품 가격 절하와 성능 향상`은 분석 성능에 영향\n",
    ">> - `통계학에 컴퓨터 공학적인 접근`을 받아들이게 하고, `통계와 기계학습 영역이 겹합되어 시너지`를 이루게 됨\n",
    "\n",
    "---\n",
    "\n",
    "- **통계학습(Statistical Learning) vs 기계학습(Machine Learning):** 알고리즘 생성 방식\n",
    "\n",
    "<center><img src='Image/Expert/StatisticsMachinelearning.png' width='800'>(루이, 다빈치랩스)</center>\n",
    "\n",
    "> **\"데이터를 통해 문제 해결한다는 점은 일맥상통하나, 해결하는 `목표/전략/방식에 대한 출발점이 다르며` 점차 `경계가 모호`해지고 있음\"**\n",
    "> - **통계학습:**\n",
    ">> - 인공지능 및 기계학습에 대한 `통계의 대응`\n",
    ">> - 기술통계, 추론통계에서 나아간 개념이지만 엄밀히 말하면 결국 `추론 통계`\n",
    ">> - `실패`의 위험을 줄여 신뢰성을 높이는 것\n",
    ">> - `모델기반` 사고방식과 `데이터기반` 사고방식을 모두 활용\n",
    ">> - `정확도` 자체에 매몰되지 않고 `모델설명력`과 다양한 `가정` 고려\n",
    ">> - 모델들이 대부분 내부 구조 파악이 쉬운 `화이트박스(White-box)` 모형/알고리즘\n",
    ">\n",
    "> - **기계학습:**\n",
    ">> - `인공지능, 패턴인식 등`의 발전 역사와 결이 같음\n",
    ">> - 에이전트라는 `인간과 비슷한 녀석`이 사물을 보고, 듣고, 인식하게 하는 것을 목적으로 발전\n",
    ">> - `성공`의 확률을 높이는 것\n",
    ">> - 인과성보다 `정확도`에 굉장히 의존적이고 모델이나 가정에 크게 관심 없음\n",
    ">> - 모델들의 내부 구조를 속속들이 알아야 할 이유가 없고 `맞추면 장땡`\n",
    ">> - 모델 내부 구조를 알 수가 없는 `블랙박스(Black-box)` 모형/알고리즘\n",
    "\n",
    "|  \t| **통계학습(Statistical Learning)** \t| **기계학습(Machine Learning)** \t|\n",
    "|:---:\t|:---:\t|:---:\t|\n",
    "| **이론적 배경** \t| `통계학` \t| `컴퓨터과학` \t|\n",
    "| **발전 기반** \t| 통계학, 수치해석 등 \t| 패턴인식, 인공지능 등 \t|\n",
    "| **모형 구조** \t| 대부분 `화이트박스` \t| 대부분 `블랙박스` \t|\n",
    "| **관심 목적** \t| `설명력`, `실패위험` 줄이기 \t| `정확성`, `성공확률` 높이기 \t|\n",
    "| **주 사용 데이터** \t| 관측치 및 변수가 적은 경우 \t| 관측치 및 변수가 많은 경우 \t|\n",
    "| **상황/가정 반영** \t| 의존적<br>(독립성, 정규성, 등분산성 등) \t| 독립적<br>(대부분 무시) \t|\n",
    "| **학습 방법** \t| 데이터에 맞게 `최적화 중점` \t| `반복학습으로 모델 구축` 중점 \t|\n",
    "| **성능 평가** \t| 데이터의 해석과 가정 적합성 등 \t| 분할 데이터 반복 평가 \t|\n",
    "| **특징** \t| 가설(Hypothesis), 모집단(Population), 표분(Sample)에 기반하여 <br>데이터를 기술(Descriptive)하거나 추론(Inference)하는데 이용 \t| 예측력(Prediction) 중심의 다양한 문제 해결을 위한 <br>지도(Supervised), 비지도(Unsupervised), 강화학습(Reinforcement) 등의 방법론 구축에 이용 \t|\n",
    "| **문제 예시** \t| 대기오염과 호흡기 질환의 관계<br>     배너위치에 따른 컨텐츠 클릭 빈도 변화<br>     신규 장비의 불량률 감소 효과 분석<br>     임상을 통한 신약의 효능 분석 \t| 이미지 데이터의 객체 구분<br>     상황이나 사물인식 성능 향상<br>     음성인식을 통한 AI스피커 성능 향상<br>     MRI데이터 사용 암 환자 조기 진단 \t|\n",
    "\n",
    "<center><img src='Image/Expert/DL_AutoFE.PNG' width='600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형회귀분석의 신경망 표현\n",
    "\n",
    "**1) 알고리즘 함수세팅:** \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Y \\approx \\hat{Y} &= f(X_1, X_2, ..., X_k) = w_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \n",
    "= [w_0~w_1~w_2~\\cdots~w_k]\\begin{bmatrix} 1 \\\\ X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_k \\end{bmatrix} \\\\\n",
    "&= [1~X_1~X_2~\\cdots~X_k]\\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_k \\end{bmatrix} \n",
    "= \\begin{bmatrix} 1~X_{11}~X_{21}~\\cdots~X_{k1} \\\\ 1~X_{12}~X_{22}~\\cdots~X_{k2} \\\\ \\vdots \\\\ 1~X_{1t}~X_{2t}~\\cdots~X_{kt} \\end{bmatrix}\n",
    "\\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_k \\end{bmatrix} = XW = WX\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Expert/Example_LinearRegression.png' width='900'></center>\n",
    "\n",
    "**2) 함수 추정을 위한 비용함수:** 나의 주장 기반 알고리즘의 `예측값`($\\hat{Y}$)과 `실제 데이터`($Y$)의 차이를 평가하는 함수\n",
    "> - **손실함수(Loss Function):** `하나의 데이터(Single Row)`에서 예측값과 정답의 차이를 평가\n",
    "> - **비용함수(Cost Function):** `모든 데이터`에서 예측값과 정답의 차이를 평가\n",
    "> \n",
    "> $$\n",
    "\\begin{aligned}\n",
    "Y - \\hat{Y} &= Y - WX = \\text{residual} = \\text{cost} \\\\\n",
    "&= \\sum_{i=1}^{m} \\left[ \\sum_{j=1}^{k} (Y_{i} - w_{j}X_{j}) \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    ">\n",
    "> - `회귀분석`은 여러가지의 비용함수 중 `최소제곱법/최소자승법`을 사용 \n",
    "> - `최소제곱법/최소자승법`를 최소로 하는 `직선`을 추정하여 `계수(coefficient)`를 결정\n",
    ">\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{W} = \\underset{W}{\\arg\\min} \\sum_{i=1}^{m} \\left[\\sum_{j=1}^{k} (Y_{i} - w_{j}X_{j})^2 \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**3) Linear Regression의 신경망 표현:** `입력값` $X$변수에 `가중치` $w$가 반영되어 `출력값` $Y$에 영향\n",
    "\n",
    "\\begin{align*}\n",
    "Y \\approx \\hat{Y} &= f(X_0, X_1, X_2, \\cdots, X_k) \\\\\n",
    "&= w_0X_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \\\\\n",
    "&= \\sum_{i=0}^{k} w_i X_i \\rightarrow \\sigma(\\sum_{i=0}^{k} w_i X_i) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "<center><img src='Image/Expert/DL_Comparing1.PNG' width='250'></center>\n",
    "\n",
    "\\begin{align*}\n",
    "&(1)~X: 입력층 \\\\\n",
    "&(2)~w_i: 가중치 \\\\\n",
    "&(3)~\\sigma: 활성함수(Linear) \\\\\n",
    "&(4)~Y: 출력층 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱회귀분석의 신경망 표현\n",
    "\n",
    "**1) 알고리즘 함수세팅:** 분류문제를 푸는 대표적인 알고리즘 `Logistic Regression`\n",
    "\n",
    "- `범주형 종속변수`의 적합/추정하기 위한 `변환과정` 필요\n",
    "- `Logistic/Sigmoid Function`를 사용하여 `곡선(S-curve) 형태로 변환`\n",
    "\n",
    "<center><img src='Image/Expert/Linear_Logistic.png' width='600'></center>\n",
    "\n",
    "> **(1) 회귀분석 추정:**\n",
    ">\n",
    ">\\begin{align*}\n",
    "Y \\approx \\hat{Y} &= f(X_1, X_2, ..., X_k) \\\\\n",
    "&= w_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \\\\\n",
    "&= XW\n",
    "\\end{align*}\n",
    ">\n",
    "> **(2) 시그모이드 변환(Logistic/Sigmoid Transformation):** `Binary Classification` 반영하는 `곡선 형태`로 변경\n",
    ">\n",
    ">\\begin{align*}\n",
    "Pr(\\hat{Y}) &= \\dfrac{1}{1+exp(-\\hat{Y})} \\\\ \n",
    "&= \\dfrac{1}{1+exp(-XW)} \\\\\n",
    "&= \\dfrac{exp(XW)}{1+exp(XW)}\n",
    "\\end{align*}\n",
    ">\n",
    "> **(3) 로짓 변환(Logit Transformation):** `X`의 선형관계 형태로 변환하여 `변수들`로 `Y=1`인 확률 추정\n",
    ">\n",
    ">\\begin{align*}\n",
    "Pr(\\hat{Y}) \\left( 1 + exp(XW) \\right) &= exp(XW) \\\\\n",
    "Pr(\\hat{Y}) &= \\left( 1 - Pr(\\hat{Y}) \\right) exp(XW) \\\\\n",
    "\\text{Odds(ratio):} \\left( \\dfrac{Pr(\\hat{Y})}{1 - Pr(\\hat{Y})} \\right) &= exp(XW) \\\\\n",
    "\\text{Logit(log-odds): } log \\left( \\dfrac{Pr(\\hat{Y})}{1 - Pr(\\hat{Y})} \\right) &= XW = w_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \\\\\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "**2) 함수 추정을 위한 비용함수:** 나의 주장 기반 알고리즘의 `분류값`($Pr(\\hat{Y})$)과 `실제 데이터`($Y$)의 차이를 평가하는 함수\n",
    "\n",
    "- **이슈: `잔차`를 사용하는 Linear Regression 비용함수 적용 어려움**    \n",
    "\n",
    "> (1) 분류문제에서는 $\\hat{Y}$를 사용한 `잔차(에러)계산이 무의미`   \n",
    ">\n",
    "> (2) 잔차($Y - \\hat{Y}$)를 `시그모이드 및 로짓 변환`을 하면 Non-convex 형태가 되서 `최소값(Global Minimum) 추정 어려움`    \n",
    ">\n",
    "> (3) 정확한 `수학적 방정식 기반` 계수추정 어렵기에 `확률론적 접근 필요`    \n",
    ">\n",
    "> <center><img src='Image/Expert/Cost_Comparison.png' width='600'></center>\n",
    "\n",
    "- **방향:** 회귀문제와 달리 `새로운 비용함수`가 필요\n",
    "\n",
    "> - Y를 `잘` 분류하면 `cost=0`으로 그렇지 않으면 cost=$\\infty$가 되는 방향\n",
    ">> - (빨간선) 실제값이 `1`일때 예측값이 `1`이면 Cost는 `0`\n",
    ">> - (빨간선) 실제값이 `1`일때 예측값이 `0`이면 Cost는 `무한대`\n",
    ">\n",
    "> \\begin{align*}\n",
    "\\text{Cost} = \\begin{cases} -log(Pr(\\hat{Y})) ~~~~ & \\text{in the case of } ~~~ Y = 1 \\\\ -log(1-Pr(\\hat{Y})) ~~~~ & \\text{in the case of } ~~~ Y = 0 \\end{cases}\n",
    "\\end{align*}\n",
    "> <center><img src='Image/Expert/Cost_Logistic.png' width='600'></center>\n",
    "\n",
    "- **Cross Entropy 등장:** Y가 0과 1인 경우의 `Cost를 결합`하여 하나의 식으로 표현\n",
    "\n",
    "> - 로지스틱 알고리즘은 `비용함수`로 `Cross Entropy`를 사용하고 `최소로 하는 계수/가중치 추정`\n",
    "> - `Y=0`인 경우 `파란부분`만 남고 `Y=1`인 경우 `빨간부분`만 남아 `Class별`로 독립적으로 작동\n",
    "> - 분류문제의 `Cost 함수는 다양`하고 많지만 통계학적으로 `Cross Entropy`는 계수 추정에 `효율적`인 편\n",
    "> - `Convex 형태`이기 때문에 `Global Minimum`을 찾기가 용이함\n",
    "> - 추정된 계수/가중치($\\hat{w}$)로 방정식을 만들어 Y=1인 `분류확률` 계산 가능\n",
    ">\n",
    ">\\begin{align*}\n",
    "\\text{Cost} &= \\sum_{i=1}^{m} \\left[ - \\color{red}{\\hat{Y}_{i} log (Pr(\\hat{Y}_{i}))} - \\color{blue}{(1-\\hat{Y}_{i}) log (1-Pr(\\hat{Y}_{i}))} \\right] \\\\\n",
    "\\hat{W} &= \\underset{W}{\\arg\\min} \\sum_{i=1}^{m} \\left[\\text{Cost} \\right] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "**3) Logistic Regression의 신경망 표현:** Linear Regression를 실행한 후 `활성함수를 Logistic/Sigmoid로 반영`\n",
    "\n",
    "\\begin{align*}\n",
    "Y \\approx \\hat{Y} &= f(X_0, X_1, X_2, \\cdots, X_k) \\\\\n",
    "&= w_0X_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \\\\\n",
    "&= \\sum_{i=0}^{k} w_i X_i \\rightarrow \\sigma(\\sum_{i=0}^{k} w_i X_i) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "<center><img src='Image/Expert/DL_Classification.png' width='450'></center>\n",
    "\n",
    "\\begin{align*}\n",
    "&(1)~X: 입력층 \\\\\n",
    "&(2)~w_i: 가중치 \\\\\n",
    "&(3)~\\sigma: 활성함수(Logistic/Sigmoid) \\\\\n",
    "&(4)~Y: 출력층 \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다층신경망 구조로의 일반화\n",
    "\n",
    "**1) 은닉층이 반영된 신경망 구조:** \n",
    "\n",
    "<center><img src='Image/Expert/DL_Comparing2.PNG' width='400'></center>\n",
    "\n",
    "> **(1)** `입력값` $X$변수에 `가중치` $w$가 반영되어 `은닉층` $H$에 영향\n",
    ">\n",
    "> **(2)** `은닉층` $H$변수에 `가중치` $v$가 반영되어 `출력층` $Y$에 영향  \n",
    "\n",
    "\\begin{align*}\n",
    "Y \\approx \\hat{Y} &= f(H_0, H_1, \\cdots, H_{k-2}) \\\\\n",
    "&= v_0H_0 + v_1H_1 + \\cdots + v_{k-2} H_{k-2} \\\\\n",
    "&= v_0h_0(X_{0}, X_{1}, \\cdots, X_{k}) + v_1h_1(X_{0}, X_{1}, \\cdots, X_{k}) + \\cdots + v_{k-2} h_{k-2}(X_{0}, X_{1}, \\cdots, X_{k}) \\\\\n",
    "&= v_0(w_{00}X_{0} + w_{10}X_{1} + \\cdots + w_{k0} X_{k}) + v_1(w_{01}X_{0} + w_{11}X_{1} + \\cdots + w_{k1} X_{k}) + \\\\\n",
    "&\\cdots + v_{k-2}(w_{0k-2}X_{0} + w_{1k-2}X_{1} + \\cdots + w_{kk-2} X_{k}) \\\\\n",
    "&= \\sum_{i=0}^{k-2} v_i ( \\sum_{j=0}^{k} w_j X_j ) \\rightarrow \\sigma_{v}(\\sum_{i=0}^{k-2} v_i \\sigma_{w} ( \\sum_{j=0}^{k} w_j X_j )) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "&(1)~X: 입력 노드 \\\\\n",
    "&(2)~w_i: 가중치(입력 노드) \\\\\n",
    "&(3)~\\sigma_{w}: 활성함수 \\\\\n",
    "&(4)~H: 히든 노드 \\\\\n",
    "&(5)~v_i: 가중치(히든 노드) \\\\\n",
    "&(6)~\\sigma_{v}: 활성함수 \\\\\n",
    "&(7)~Y: 출력 노드 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정확성 vs. 설명력\n",
    "\n",
    "|  \t| **통계학습(Statistical Learning)** \t| **기계학습(Machine Learning)** \t|\n",
    "|:---:\t|:---:\t|:---:\t|\n",
    "| **이론적 배경** \t| `통계학` \t| `컴퓨터과학` \t|\n",
    "| **발전 기반** \t| 통계학, 수치해석 등 \t| 패턴인식, 인공지능 등 \t|\n",
    "| **모형 구조** \t| 대부분 `화이트박스` \t| 대부분 `블랙박스` \t|\n",
    "| **관심 목적** \t| `설명력`, `실패위험` 줄이기 \t| `정확성`, `성공확률` 높이기 \t|\n",
    "| **주 사용 데이터** \t| 관측치 및 변수가 적은 경우 \t| 관측치 및 변수가 많은 경우 \t|\n",
    "| **상황/가정 반영** \t| 의존적<br>(독립성, 정규성, 등분산성 등) \t| 독립적<br>(대부분 무시) \t|\n",
    "| **학습 방법** \t| 데이터에 맞게 `최적화 중점` \t| `반복학습으로 모델 구축` 중점 \t|\n",
    "| **성능 평가** \t| 데이터의 해석과 가정 적합성 등 \t| 분할 데이터 반복 평가 \t|\n",
    "| **특징** \t| 가설(Hypothesis), 모집단(Population), 표분(Sample)에 기반하여 <br>데이터를 기술(Descriptive)하거나 추론(Inference)하는데 이용 \t| 예측력(Prediction) 중심의 다양한 문제 해결을 위한 <br>지도(Supervised), 비지도(Unsupervised), 강화학습(Reinforcement) 등의 방법론 구축에 이용 \t|\n",
    "| **문제 예시** \t| 대기오염과 호흡기 질환의 관계<br>     배너위치에 따른 컨텐츠 클릭 빈도 변화<br>     신규 장비의 불량률 감소 효과 분석<br>     임상을 통한 신약의 효능 분석 \t| 이미지 데이터의 객체 구분<br>     상황이나 사물인식 성능 향상<br>     음성인식을 통한 AI스피커 성능 향상<br>     MRI데이터 사용 암 환자 조기 진단 \t|\n",
    "\n",
    "---\n",
    "\n",
    "**1) 기계학습 활용 데이터분석의 현실:**\n",
    "\n",
    "> **\"(이상적으로) 머신러닝 알고리즘에 데이터를 `학습/적합/모델링` 한다는 건..\"**     \n",
    ">\n",
    "> (1) 사람/사물/시스템이 `어떻게 동작하는지 이해`의 과정    \n",
    "> (2) 사람/사물/시스템이 만들어내는 `데이터를 체계적으로 요약`하는 과정    \n",
    "> (3) 미래의 예측값과 실제값의 비교로 `사람/사물/시스템을 일반화`하는 과정    \n",
    "> (4) 일반화된 사람/사물/시스템으로 더욱 `효과적이고 체계적인 의사결정`하는 방법    \n",
    "\n",
    "> **\"(현실적으로) 많은 시간과 비용이 투입되지만, `우리 사회를 이해((1),(2),(3)) <<< 빠른 의사결정(4)`에 집중되어 효과는 글쎄..\"**\n",
    ">\n",
    "> - 사회를 이해하기 위한 `사회학/교육학/철학/경제학 등`의 학문은 `((1),(2),(3))에 집중`\n",
    "> - `경영과학/컴퓨터과학/산업공학 등`의 학문은 기술적인 자동화나 인공지능을 반영하는 `(4)에 집중`\n",
    "> - 학문적 출신(?)에 따라 `분석에 대한 관점 차이` 또는 `비즈니스 방향 차이`가 존재할 수 있음\n",
    ">\n",
    "> <center><img src='Image/Expert/Programming_Multidisciplinary.png' width='500'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**2) `정확성 vs. 설명력`은 모두 욕심낼 수 없는 반비례 관계:**\n",
    "- 대부분의 `기계학습 및 딥러닝 모델`은 이론적 기반이 없기 때문에 `1회성 추정을 반복`하는 알고리즘  \n",
    "- `통계추론`은 이론적 기반의 `결과의 범위(신뢰구간)와 설명력`을 제공하기에 `반복추정이 필요없는` 알고리즘\n",
    "\n",
    "<center><img src='Image/Expert/Performance_Explanability.png' width='600'></center>\n",
    "\n",
    "- **설명력 최근 연구동향:**   \n",
    "> - [LIME](https://blog.fastforwardlabs.com/2017/09/01/LIME-for-couples.html)   \n",
    "> - [DARPA](https://bdtechtalks.com/2019/01/10/darpa-xai-explainable-artificial-intelligence/)   \n",
    "\n",
    "---\n",
    "\n",
    "**3) 반비례 관계 원인:** `회귀분석(통계학습) vs 딥러닝(기계학습)`\n",
    "\n",
    "- **회귀분석(통계학습):** 함수의 `선형성`을 추정하는 정확성 보다 `설명력에 집중`하는 알고리즘\n",
    "- **딥러닝(기계학습):** 함수의 `비선형성`을 추정하는 설명력 보다 `정확성에 집중`하는 알고리즘\n",
    "\n",
    "| -                            | **회귀분석**                                            | **딥러닝**                                   |\n",
    "|----------------------------- |:----------------------------------- |:----------------------------------- |\n",
    "| **모델특징**                 | -                                                       | -                                                      |\n",
    "| 분석목적                     | 선형성파악(설명가능)                                    | 비선형성파악(설명불가)                                 |\n",
    "| 이론적(수학적) 근거          | 존재                                                    | 미존재                                                 |\n",
    "| **분석단계 특징(전처리)**    | -                                                       | -                                                      |\n",
    "| 데이터 로딩                  | <span style=\"color:blue\">Panel Data</span>              | <span style=\"color:red\">다양(운이좋으면 Panel)</span>  |\n",
    "| 데이터 빈칸 채우기/삭제      | <span style=\"color:red\">분석필요</span>                 | <span style=\"color:red\">분석필요</span>                |\n",
    "| 데이터 컬럼 추가/삭제        | <span style=\"color:red\">분석필요+민감</span>            | <span style=\"color:red\">분석필요+덜민감</span>         |\n",
    "| 데이터 분리                  | <span style=\"color:blue\">Train/Validate/Test</span>     | <span style=\"color:blue\">Train/Validate/Test</span>    |\n",
    "| 데이터 스케일링              | <span style=\"color:red\">분석필요/미필요</span>          | <span style=\"color:red\">분석필요</span>                |\n",
    "| **분석단계 특징(모델링)**    | -                                                       | -                                                      |\n",
    "| 입력 확인 및 변환            | <span style=\"color:blue\">Panel Data</span>              | <span style=\"color:red\">다양(정해지지 않음)</span>     |\n",
    "| 데이터 모델연결              | <span style=\"color:blue\">자동화</span>                  | <span style=\"color:red\">반자동화</span>                |\n",
    "| 비용함수(Cost)               | <span style=\"color:blue\">최소제곱에러(MSE)</span>       | <span style=\"color:red\">다양</span>                    |\n",
    "| 추정함수(Optimizer)          | <span style=\"color:blue\">고정(미분1회 대체가능)</span>  | <span style=\"color:red\">다양(미분지속)</span>          |\n",
    "| **분석단계 특징(검증)**      | -                                                       | -                                                      |\n",
    "| 정확성지표                   | <span style=\"color:red\">다양</span>                     | <span style=\"color:red\">다양</span>                    |\n",
    "| 잔차진단활용                 | <span style=\"color:red\">가능(분석필요)</span>           | <span style=\"color:blue\">불가</span>                   |\n",
    "| **분석단계 특징(결과해석)**  | -                                                       | -                                                      |\n",
    "| 관계성 시각화/영향력 해석    | <span style=\"color:red\">가능(분석필요)</span>           | <span style=\"color:blue\">불가</span>                   |    \n",
    "\n",
    "---\n",
    "\n",
    "**4) 활용 장단점 비교 요약:**\n",
    "\n",
    "<center><img src='Image/Expert/DL_Comparing2.PNG' width='400'></center>\n",
    "\n",
    "|  | **회귀분석** | **딥러닝** |\n",
    "|:-:|:-:|:-:|\n",
    "| **구조** | `은닉층이 없고` 입력층과 출력층이 다이렉트로 연결 | 입력층과 출력층이 `은닉층을 통해` 복잡한 비선형성으로 연결 |\n",
    "| **패턴 반영 횟수**<br>(모델수) | `1개`의 회귀분석 | `2개 이상(은닉층과 노드 갯수에 따라)`의 회귀분석 |\n",
    "| **설명성** | `가능`<br>(입력값과 출력값의 영향을 명확히 설명) | `불가능`<br>(알파고의 우승 이유를 모르듯 사람이 이해하기 어려움) |\n",
    "| **설명성 예시**<br>(부동산가격<br>영향요인) | **[결과]**<br>- 공원의 개수가 `2만큼 영향`<br>- 지하철역 유무가 `8만큼 영향`<br>- 유흥업소 개수가 `-3만큼 영향`<br><br>**[인싸이트]**<br>- 지하철 유무가 `큰 영향`을 주며 <br>공원과 유흥업소는 `반대의 영향을 주지만 크기는 비슷`<br>- `정확도는 70%`이기에 다른 케이스에선 <br>`30% 정도 오차` 발생 가능 | **[결과]**<br>- 공원의 개수가 1번 은닉층에서 `0.5만큼 영향`<br>- 2번 은닉층에선 `-1만큼 영향`<br>- 3번 은닉층에선 `0.1만큼 영향`<br>- 집값에는 1번 은닉층이 `1만큼 영향`<br>- 2번 은닉은 `3만큼 영향`<br>- `…`<br><br>**[인싸이트]**<br>- `그냥 모르겠음`<br>- `정확도는 95%`이기에 다른 케이스에선 <br>`동일 데이터로 정확하게 예측` 가능 |\n",
    "| **요약** | 단 `1개의 수학적 표현/가설/Layer`로<br>입력 데이터에 가장 적합한 `수학적 표현(Parameters) 추정` | `여러개의 수학적 표현/가설/Layer`로 입력데이터에<br>가장 적합한 수학적 표현을 `단계별 업데이트 추정`<br>$\\rightarrow$ Layer가 늘어날수록 더 복잡한 패턴(특징) 반영 가능<br>$\\rightarrow$ Layer 1개는 1차방정식이지만 Layer 2개는 2차방정식 |\n",
    "| **한계** | `설명은 되지만 미래에 나타나기 어려운 패턴만 모델링` | `설명은 안되지만 미래에 나타날 수 있는 패턴을 모델링`<br>- 많은 다층신경망도 `성능이 낮을 수 있기에(과적합)`,<br>모델을 간소화 하면서도 `성능을 높일 방법을 찾아야`<br>- `얼마나 많은 층을 사용해야하는지` 예측 불가<br>- `사람이 설정해줘야 하는 많은 Hyper-parameter` 존재 |\n",
    "\n",
    "- 데이터분석 프로세스는 동일하나 `딥러닝 알고리즘이 자유도가 높음`\n",
    "- 자유도는 높으나 그만큼 `분석가가 고려해야 할 것들이 많고 명확한 설명이 어려워 결과 신뢰성 어려움`\n",
    "- 단, 고려해야 할 것들을 `엄격하고 객관적으로 반영한다면 설명이 어려워도 신뢰성은 높일 수 있음`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어려움에도 딥러닝이 필요한 이유\n",
    "\n",
    "**1) 머신러닝 방향 및 발전**\n",
    "\n",
    "|  | **통계추론** | **기계학습** | **딥러닝** |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| **설명** | - 정보요약을 위한 `기술통계`<br>- `가설 기반 수학적 검정`<br>- `수치기반 예측통계` | - `데이터의 패턴을 찾기 위한 컴퓨터 알고리즘`<br>- `예측, 분류, 군집화, 분류, 시계열 분석 등` | - 인간 `뇌구조를 모방한 신경망`<br>- `인공지능(AI)의 출발점` |\n",
    "\n",
    "\n",
    "> **\"기존 분석은 `인간지능에 의해 분석된 고정 형태의 시스템`이었다면, 최근 분석은 `지속적이고 반복적이며 자동수행되는 학습을 기반으로 한 진화되는 성능개선 시스템` 방향\"**\n",
    ">\n",
    "> **(1) 통계추론과 기계학습 관계:** 통계적 분석기법 중 `회귀분석은 인공신경망과 지도학습 알고리즘으로 발전`\n",
    ">\n",
    "> **(2) 데이터마이닝과 기계학습 관계:** `데이터의 패턴과 지식 발견`을 위해 고객관계과리, 마케팅 영역의 군집화, 분류, 예측, 시계열 분석 등이 `지도학습/비지도학습으로 발전`  \n",
    ">\n",
    "> **(3) 기계학습과 인공신경망 관계:** 인간 뇌구조를 모방하여 `신경망의 층을 깊게 네트워크화 한 딥러닝 알고리즘으로 문제해결 성능향상`  \n",
    "\n",
    "---\n",
    "\n",
    "**2) 딥러닝 적용 필요성:**\n",
    "\n",
    "**(1) 딥러닝은 불완전하고 전처리없는 `Raw 데이터`에서 `자동으로 변수들을 추출`하고 학습 가능**\n",
    "\n",
    "- **기존:** 사람이 `이론과 경험을 바탕으로 직접 생성`\n",
    "\n",
    "> - **Hand-crafted Feature:** 통계추론이나 기계학습시 문제 해결을 위해 `수동으로 생성한 변수`로 일반적으로 `이론이나 사람의 경험을 기반`으로 추정\n",
    "\n",
    "- **한계:** 큰 정보를 압축하는 과정에서 `많은 정보 손실이나 오류반영 가능성`\n",
    "\n",
    "> - 이론이나 사람의 경험 기반 변수들은 적은 양의 데이터로 큰 정보를 압축하여 생성했기 때문에, 알고리즘이 `학습하기 힘든 데이터의 특징을 찾아내기도` 하지만 알고리즘이 `데이터의 특징을 학습하기 힘들게 할 수도` 있음\n",
    "> - `사람의 영향을 최소화` 하기 위해 `변수추출을 알고리즘에 맡기려고` 하지만 아직까진 완벽하진 않음\n",
    "\n",
    "- **방향:** `사람의 영향을 최소화` 및 Layer 적용을 통해 `새로운 Feature Extraction`\n",
    "\n",
    "> - `CNN`의 Layer, `Autoencoder`의 압축되어 있는 Representation Vector, `Embedding 등`\n",
    "\n",
    "<center><img src='Image/Expert/DL_AutoFE.png' width='600'></center>\n",
    "\n",
    "**(2) 데이터 형태와 갯수에 상관없이 약간의 처리를 통해 딥러닝은 `여러개의 입력을(Multiple Input) 받아 여러개의 출력을(Multivariate Output) 쉽게 가능`하게 함**\n",
    "\n",
    "- **기존 및 한계:** 기계학습 알고리즘도 다양한 입출력을 사용할 순 있지만 `알고리즘에 따라 제한적`\n",
    "\n",
    "- **방향:** 입출력에 따른 `알고리즘 구분 없이 딥러닝 구조의 처리를 통해 모든 경우 가능`\n",
    "\n",
    "**(3) 비교적 `길이가 긴 시퀀스(Sequence)에 걸쳐있는 데이터 패턴` 추출도 가능**\n",
    "\n",
    "- **기존 및 한계:** 입력되는 데이터가 길수록(시간) `추정해야 할 파라미터 양과 연산시간이 급격히 늘어나고`, 가중치 파라미터가 `유일하지 않기 때문에 결과 신뢰성 이슈`\n",
    "\n",
    "- **방향:** 알고리즘 `내부에서 이전 시점들의 데이터 학습을 별도로 반영`\n",
    "\n",
    "> - RNN 계열은 Cell 내부에서 이전 시점의 데이터들을 기억하도록 반영하여 `계절성`과 같은 특유의 `긴 Sequence로 분포된 데이터 패턴` 학습 가능\n",
    "\n",
    "---\n",
    "\n",
    "**3) 일반적 알고리즘의 한계 인지: `Garbage in, garbage out`**\n",
    "\n",
    "- 대부분 연구들은 `알고리즘 성능에 집중`하고 `어떻게 분석 했는지 없이 그저 딥러닝 결과가 좋았다!`      \n",
    "\n",
    "- `실제 분석에 활용`해보면 오히려 `통계추론이나 기계학습 보다 성능 낮은 경우 다수`\n",
    "\n",
    "- 실제 실리콘밸리의 많은 회사들은 `딥러닝이 아닌 회귀분석으로 수익`을 내고 있다 (Andrew Ng 교수)      \n",
    "\n",
    "> **\"결국 핵심은 `알고리즘이 아니라 데이터!`, `데이터가 Garbage인데 알고리즘이 아무리 좋아봤자 Garbage를 출력`한다\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **딥러닝 알고리즘 (Deep Learning)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN계열 vs RNN계열\n",
    "\n",
    "> **\"`문제 유형`에 따라 적절한 아키텍쳐 선택!\"**\n",
    "\n",
    "<center><img src='Image/Expert/DL_Comparing_CNNRNN.png' width='800'></center>\n",
    "\n",
    "| 데이터 | 분야 | 알고리즘 |\n",
    "|:-:|:-:|:-:|\n",
    "| **스냅샷<br>(Snapshot)** | `이미지, 영상, 바둑` (시간 무관) | CNN계열 |\n",
    "| **시퀀스<br>(Sequence)** | `기상, 주가, 언어, 음성` (시간 연관) | RNN계열 |\n",
    "\n",
    "---\n",
    "\n",
    "**1) CNN:** `이미지`처럼 `여러 값뭉치로 패턴이 존재하는 데이터를 처리하는데 특화`된 모델\n",
    "\n",
    "- 이미지나 영상에서의 `인식이나 분류 문제 등`에서 뛰어난 결과\n",
    "- 입력 데이터보다 `더 큰 데이터의 패턴으로 손쉽게 확장`될 수 있는 특징\n",
    "- 각 `은닉층 노드`의 연산이 `한번만 실행`\n",
    "\n",
    "> - `Lenet5 (1998)`\n",
    "> - `Alexnet (2012)`\n",
    "> - `ZFNet (2013)`\n",
    "> - `VGGNet (2014)`\n",
    "> - `GoogLeNet (2014)`\n",
    "> - `ResNet (2015)`\n",
    "\n",
    "**2) RNN:** `순서가 있는 데이터(시계열, 자연어 등)를 처리하는데 특화`된 모델\n",
    "\n",
    "- `예측`이 가장 큰 분석 목적이자 활용분야\n",
    "- `비현실적으로 긴 시계열`도 쉽게 확장 가능\n",
    "- `가변 길이의 시계열 데이터`도 처리 가능\n",
    "- 각 `은닉층 노드를 공통으로 사용`하여 계속 갱신\n",
    "\n",
    "> - `RNN (1986)`\n",
    "> - `LSTM (1997)`\n",
    "> - `GRU (2014)`\n",
    "> - `Sequence-to-Sequence (Seq2Seq, 2014)`\n",
    "> - `Attention (2015)`\n",
    "> - `Transformer (2017)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Neural Network\n",
    "\n",
    "> **\"데이터의 `의미(Feature/Variable)를 학습하면 비효율적`이고 데이터의 `특징/패턴을 별도 추출`하여 학습하는 신경망 (LeCun et al. 1989)\"**\n",
    ">\n",
    "> - 뇌가 `시각정보를 처리`하기 위해 시각피질을 구성하고 있는 `망막(입력층) -> 단순세포(은닉층) -> 복합세포(출력층)의 기능을 모형`으로 만든 것\n",
    ">\n",
    "> <center><img src='Image/Expert/CNN_Learning_Ideation.PNG' width='1000'></center>\n",
    ">\n",
    "> - 일반적으로 `인접한 픽셀간의 상관성`이 있는데 `벡터화 과정에서 정보 손실` 발생\n",
    ">\n",
    "> <center><img src='Image/Expert/CNN_Vectorization.PNG' width='1000'>(https://sungwookkang.com/1408)</center>\n",
    ">\n",
    "> - 사진 속의 이미지가 `새 라는 것을 인식`해야 하는데 `주변 배경 데이터는 학습할 필요가 없음`\n",
    "> - **CNN 구조:** `1) Convolution Layer + 2) Pooling Layer + 3) Fully Connected Layer & Activation`\n",
    ">\n",
    "> <center><img src='Image/Expert/CNN_Process_Example.webp' width='1000'>(https://towardsdatascience.com/deep-learning-2-f81ebe632d5c)</center>\n",
    "\n",
    "---\n",
    "\n",
    "**1) Convolution Layer:** \n",
    "\n",
    "- 데이터 각 성분의 `인접 주변 정보들을 포함`하여 데이터의 `주요특징을 추출`하여 `한장으로 요약`하는 Layer\n",
    "- 데이터를 `압축/요약하는 과정에서 사용되는 필터를 공유`하여 DNN 대비 `파라미터의 갯수가 현저히 적어서 과적합 문제 덜 발생`\n",
    "- 요약된 정보를 통상 `Convolution Layer`라고, 엄밀히 말하면 `Cross-Correlation(교차상관)`를 사용하여 연산량을 줄이는 방식 사용\n",
    "- `입력데이터` $\\rightarrow$ `채널(Channel) 확인` $\\rightarrow$ `필터(Filter)/Stride/패딩(Padding) 결정` $\\rightarrow$ `각 채널 별 합성곱(Convolution)으로 특성추출`\n",
    "\n",
    "<center><img src='Image/Expert/CNN_Convolution_Process.PNG' width='700'>(https://chacha95.github.io/2018-12-02-Deeplearning3/)</center>\n",
    "\n",
    "> **(1) 채널(Channel):** `입력데이터의 깊이`를 의미\n",
    ">\n",
    "> - `정형데이터=1채널, 측백이미지=1채널, 컬러이미지=3채널, 시간데이터=시간인텍스 수만큼의 채널`\n",
    "> - 채널이 여러개인 경우 `각 채널별 특성을 추출한 후 결합`시키는 과정 포함\n",
    ">\n",
    "> **(2) 필터(Filter)/Stride/Padding:** `데이터/이미지의 특징을 추출`하기 위한 행렬\n",
    ">\n",
    "> - 필터의 크기는 정해지지 않았으며, 일반적으로 `3*3, 4*4, 5*5 등`의 정사각형 사용\n",
    "> - 만약 `필터의 크기가 입력데이터의 크기와 같다면 DNN과 동일`\n",
    "> - `입력데이터에서 필터의 크기만큼의 데이터(파란색 네모칸)를 추출`하여 `필터를(보라색) 곱해서 합`하여 특성 값 추출\n",
    "> - 필터를 적용한다는 것은, `회귀분석을 1회 실시하는 것 또는 업데이트가 필요한 가중치 파라미터 추정 `\n",
    "> - 분석가가 필터의 값들을 `임의의 랜덤값으로 초기화` 후 `데이터 학습을 통해 최적의 값으로 업데이트`\n",
    "> - 필터 사용을 통해 `가중치를 공유`하게 되고 `필요한 가중치의 수가 대폭 줄어 연산량 감소`\n",
    ">\n",
    "> <center><img src='Image/Expert/CNN_Filter.PNG' width='600'>(https://anhreynolds.com/blogs/cnn.html)</center>\n",
    ">\n",
    "> - **Stride:** `필터가 특성값을 추출`하기 위해 움직이는 간격\n",
    ">\n",
    "> <center><img src='Image/Expert/CNN_Filter_Stride.jpg' width='700'>(https://wikidocs.net/152775)</center>\n",
    ">\n",
    "> - **패딩(Padding):** 데이터 유실을 줄이기 위해 `데이터 외곽에 특정 값(주로 0)으로 채우는 과정`\n",
    "> - 최종 `Feature Map의 크기는 입력데이터의 크기보다 작아지기` 떄문에 입력과 출력 크기를 같거나 비슷하게 맞추는 편\n",
    "> - 패딩이 없다면, `가장자리 부분은 내부 데이터보다 패턴 탐색의 기회가 적어지는` 효과\n",
    "> - 패딩은 사이즈 조절 이외에도 입력데이터에 `특정 값의 노이즈를 섞기 때문에 과적합을 방지`하는 효과\n",
    ">\n",
    "> <center><img src='Image/Expert/CNN_ZeroPadding.gif' width='800'>(https://medium.com/@draj0718/zero-padding-in-convolutional-neural-networks-bf1410438e99)</center>\n",
    ">\n",
    "> - **예시:** `Channel 3개 + Filter 2개 + Stride 2 + Zero Padding`\n",
    "> - 데이터의 `특징이 있다면 큰값`이 출력되고 `특징이 없다면 작은값`이 출력\n",
    "> - `Filter의 갯수`만큼 모델링되어 출력값 생성\n",
    "> - `각 채널 데이터마다 다양한 필터를 적용`해야 다양한 특성 추출\n",
    ">\n",
    "> <center><img src='Image/Expert/CNN_Convolution.gif' width='800'>(https://cs231n.github.io/convolutional-networks/)</center>\n",
    "\n",
    "---\n",
    "\n",
    "**2) Pooling Layer:**\n",
    "\n",
    "- `합성곱 출력값을 입력받아 특정 부분을 선택/강조`하여 사이즈를 줄이는 과정으로 `Sub Sampling`\n",
    "- 하나를 선택하는 방식에 따라 `Min/Average/Max Pooling 등`이 있으며 `Max Pooling`이 주로 사용\n",
    "- 데이터 `노이즈를 상쇄`시키고 `미세한 부분에서 일관적인 특징을 추출`하는 효과\n",
    "- 항상 사용하지는 않고 `데이터의 크기를 줄이고 싶을 때 선택적 사용`하며 `과적합 방지 효과`\n",
    "- `Stride & Padding`은 일종의 Feature Engineering 기술로 `컨볼루션 뿐만 아니라 풀링에서 사용 가능`\n",
    "\n",
    "<center><img src='Image/Expert/CNN_Pooling.gif' width='600'>(https://towardsdatascience.com/convolutional-neural-networks-explained-how-to-successfully-classify-images-in-python-df829d4ba761)</center>\n",
    "\n",
    "> - `학습 파라미터가 없고 출력값의 채널수의 변경도 없음`\n",
    "> - 오로지 `입력값의 크기만 감소`되어 출력\n",
    "\n",
    "---\n",
    "\n",
    "**3) Fully Connected Layer & Activation:**\n",
    "\n",
    "- 지금까지 처리된 `행렬형태의 출력값들`을 가진 `모든 노드들을 연결시켜 최종 목적에 부합하는 형태의 1차원 열벡터(One-hot Vector)로 표시`\n",
    "- 목적의 형태에 따라서 사용하지 않을 수도 있어서 `필수는 아니지만 기본적으로 사용하는 편`\n",
    "\n",
    "<center><img src='Image/Expert/CNN_Process_BlackImage.PNG' width='800'>(https://all-young.tistory.com/43)</center>\n",
    "\n",
    "> - 다중분류에 Softmax 함수를 쓰며, `목적에 맞는 적절한 활성화 함수 반영`\n",
    "\n",
    "---\n",
    "\n",
    "**4) 각 층별 학습된 패턴 예시:**\n",
    "\n",
    "<center><img src='Image/Expert/CNN_Layer2.webp' width='700'></center>\n",
    "<center><img src='Image/Expert/CNN_Layer3.webp' width='700'></center>\n",
    "<center><img src='Image/Expert/CNN_Layer4_Layer5.webp' width='700'>(https://towardsdatascience.com/deep-learning-2-f81ebe632d5c)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "\n",
    "> **\"단방향 신경망 외에 `이전 출력값이 다시 입력으로 연결되는 순환신경망`(Rumelhart et al.1986)\"**\n",
    ">\n",
    "> - 길이가 T인 시계열은 각 시간마다 은닉상태가 계산되지만, `모듈화를 생각해 하나의 층으로 구현`\n",
    ">\n",
    "> <center><img src='Image/Expert/DL_RNN_UnfoldFold.PNG' width='600'></center>\n",
    ">\n",
    "> <center><img src='Image/Expert/DL_Understand_Flow.jpg' width='600'>(http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</center>\n",
    ">\n",
    "> - **예시: `Many-to-Many`**\n",
    ">\n",
    "> <center><img src='Image/Expert/DL_RNN_Example.PNG' width='600'></center>\n",
    "> <center><img src='Image/Expert/DL_RNN_Flow_Example.png' width='500'>(http://cs231n.stanford.edu/2017/syllabus.html)</center>\n",
    "\n",
    "---\n",
    "\n",
    "**1) 순전파:**\n",
    "\n",
    "<!-- <center><img src='Image/Expert/DL_RNN_LSTM_Compare1.PNG' width='600'>(http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</center> -->\n",
    "\n",
    "- `입력과 출력 길이의 제한이 없고` 기본적으로 `Fully Connected Layer 구조`\n",
    "- `다음 시점의 예측값 확률`을 출력하며 `매 시점마다 정답에 가깝도록 가중치 평가`(초록색 강화 및 빨간색 약화)\n",
    "- `입력데이터`($x_t$)가 RNN 모형에 들어가면, `이전 은닉층값`($h_{t-1}$)과 `특정 가중치`($W, U$)로 결합하여 `새로운 은닉층값`($h_t$) 생성\n",
    "- $t$는 시간을 의미하며 `가중치들은 모든 시점들에 공유`됨\n",
    "\n",
    "> **(1) 은닉층:** $H_t = f(H_{t-1}, X_t) = \\sigma_H (U H_{t-1} + W X_t) = tanh (U H_{t-1} + W X_t)$ \n",
    ">\n",
    "> - **$X_t$:** `입력값`으로, $m(입력차원수) \\times n(배치크기)$\n",
    "> - **$W$:**: `입력값과 은닉층 사이의 가중치`로, $k(은닉차원수) \\times m(입력차원수)$\n",
    "> - **$H_{t-1}$:** `이전시점의 은닉층값`으로, $k(은닉차원수) \\times n(배치크기)$\n",
    "> - **$U$:**: `이전 은닉층과 현재 은닉층 사이의 가중치`로, $k(은닉차원수) \\times k(은닉차원수)$\n",
    "> - **$H_{t}$:** `현재시점의 은닉층값`으로, $k(은닉차원수) \\times n(배치크기)$\n",
    ">\n",
    "> **(2) 출력층:** $Y_t = f(H_T) = \\sigma_Y (VH_t) = softmax(VH_t), ~ V \\in R^{K \\times 1}$\n",
    ">\n",
    "> - **$H_{t}$:** `현재시점의 은닉층값`으로, $k(은닉차원수) \\times n(배치크기)$\n",
    "> - **$V$:**: `현재 은닉층과 출력값 사이의 가중치`로, $o(출력차원수) \\times k(은닉차원수)$\n",
    "> - **$Y_t$:** `출력값`으로, $o(출력차원수) \\times n(배치크기)$\n",
    "\n",
    "- 비선형 활성화 함수들 중 `tanh 및 softmax 사용`\n",
    "\n",
    "<center><img src='Image/Expert/DL_RNN_Activation_Without.gif' width='800'></center>\n",
    "<center><img src='Image/Expert/DL_RNN_Activation_With.gif' width='800'></center>\n",
    "<!-- (https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) -->\n",
    "\n",
    "---\n",
    "\n",
    "**2) 역전파:** `각 시점마다` 비용함수를 평가하여 최소가 될때까지 `반복적으로 가중치 업데이트`($V, U, W$)\n",
    "\n",
    "<center><img src='Image/Expert/DL_RNN_Cycle.JPG' width='900'>(https://www.techleer.com/articles/185-backpropagation-through-time-recurrent-neural-network-training-technique/)</center>\n",
    "\n",
    "---\n",
    "\n",
    "**3) 한계(Bengio et al.1994):** \n",
    "\n",
    "- **Long-term Dependency(현상):** 은닉층의 과거 정보가 `마지막까지 전달되지 못하는` 현상(성능 하락)\n",
    "\n",
    "> - $H_t = \\sigma_H (U H_{t-1} + W X_t) = \\sigma_H (U \\sigma_H (U H_{t-2} + W X_{t-1}) + W X_t)$  \n",
    ">\n",
    "> $\\rightarrow$ `마지막 단어를 예측`한다고 할 때 `초반 단어가 뒷단까지 충분히 전달되기 어려워` 예측 어려움\n",
    ">\n",
    "> <center><img src='Image/Expert/DL_RNN_LongDepend.PNG' width='600'></center>\n",
    "\n",
    "- **Vanishing Gradients(원인):** 입출력의 거리가 멀면 `역전파시 기울기가 작아지던가 커져서 학습능력 저하(RNN 구조문제)`\n",
    "\n",
    "> - 가중치의 최대값 중 하나가 `1보다 크면/작으면 폭발적으로 증가할/감소할 것`\n",
    ">\n",
    "> <center><img src='Image/Expert/DL_RNN_VanishingGradient.png' width='700'>(https://mblogthumb-phinf.pstatic.net/)</center>\n",
    ">\n",
    "> $\\rightarrow$ 기울기가 커지는 경우는 `기울기의 한계점을 두어 상한선 부여`(Gradient Clipping)\n",
    ">\n",
    "> $\\rightarrow$ 가장 일반적인 해결책은 `sigmoid나 tanh 대신 relu 사용`하는 것이지만, `RNN 계열은 같은 레이어를 반복하기 때문에 미사용`\n",
    ">\n",
    "> $\\rightarrow$ 특정 범위(보통 5-step)의 비용함수만 전파시키는 `Truncated 역전파`를 사용하거나 `LSTM 사용`\n",
    ">\n",
    "> - **Truncated Backpropagation:** `Sequence가 매우 길면` 모델링 과정의 순전파와 역전파에 `시간적 비용이 매우 크기 때문에 일정한 크기 내의 비용함수 사용`\n",
    ">\n",
    "> <center><img src='Image/Expert/RNN_TruncatedBP.jfif' width='800'>(https://noru-jumping-in-the-mountains.tistory.com/m/13)</center>\n",
    ">\n",
    "> - 장기간 메모리 유지를 위한 Cell이 반영된 `LSTM과 GRU`\n",
    ">\n",
    "> <center><img src='Image/Expert/DL_LSTM_GRU.png' width='700'>(https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory\n",
    "\n",
    "> **\"`RNN의 단기기억문제 해결책`으로 고안되었으며 당시는 관심이 낮았지만, `2000년대부터 폭발적인 관심으로 주요 RNN 모델`로 자리잡음 (Hochreiter et al. 1997)\"**  \n",
    ">\n",
    "> - `최근데이터(Short Term)와 오래전데이터(Long Term)를 함께 기억(Memory)`하여 출력\n",
    "> - 일정한 오류를 유지함으로써 여러 네트워크가 `장시간(1000개 이상)에 걸쳐 학습 가능`하도록 함\n",
    "> - `Cell 개념을 도입`하여 열리고 닫히는 게이트를 통해 `어떤 데이터를 저장/읽기/쓰기/삭제 결정`\n",
    "> - 기존 RNN에 Cell State($C$)를 추가하여 `얼마나 과거의 데이터를 기억할지` 결정\n",
    "> - RNN의 은닉층을 `LSTM Block`으로 대체되며 기존 $H$에 $C$가 추가된 네트워크 구조\n",
    ">\n",
    "> <center><img src='Image/Expert/DL_RNN_LSTM_Compare1.PNG' width='600'></center>\n",
    "> <center><img src='Image/Expert/DL_RNN_LSTM_Compare2.PNG' width='600'>(http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</center>\n",
    "\n",
    "---\n",
    "\n",
    "**1) LSTM Block:** `곱하기 이외에 더하기를 사용`하였고, 두개 가중치($C$:long-term & $H$:short-term)의 `가중평균`\n",
    "\n",
    "- `RNN 구조를 기반`으로 하되, 은닉층에서 무작정 합성곱을 하지 않고 `기억해야/잊어도 될 정보를 따로 계산`\n",
    "- 핵심은 $C$라는 `장기기억셀`을 사용하는 것으로 `LSTM 계층 내에서만 전달`되며 과거부터 $t$까지 모든 정보 저장하고 곱셈 외 덧셈을반영하여  `역전파시 기울기 소실 문제가 해결`\n",
    "\n",
    "<center><img src='Image/Expert/DL_RNN_Operation.png' width='700'>(http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</center>\n",
    "\n",
    "**(0) 게이트(Gate):** `데이터의 흐름을 제어하는 문`의 개념으로 RNN과 달리 `3개의 입력값`을 사용\n",
    "\n",
    "- $X_t, H_{t-1}$는 각각 `1) 현재의 입력과 2) 과거의 단기정보`를 의미\n",
    "- $C_t$는 `3) 과거의 장기정보`를 제어하며,\n",
    "$$C_t = F_t*C_{t-1} + I_t*G_t$$\n",
    "> - 어제까지의 데이터를 얼마나 잊을지 $\\rightarrow$ $F_t$\n",
    "> - $F_t$가 0이면 과거의 영향 미고려 & 1이면 과거의 영향이 상당한 의미\n",
    "> - 현재에서의 새로운정보($G_t$)를 얼마나 반영할지($I_t$)를 통해 `실질적 업데이트`\n",
    "\n",
    "<span style=\"color:red\">$\\rightarrow$ **\"기존의 정보를 얼만큼 잊고, 새로운 정보로 얼만큼 대체 할 것인가?\"**</span>\n",
    "\n",
    "**(1) 망각게이트(Forget Gate):** `과거(장기)를 얼마나 잊을지` 제어하기 위한 가중치\n",
    "\n",
    "- **sigmoid:** 정보가 `0과 1사이에서 얼마나 통과`시킬지 결정\n",
    "\n",
    "$$F_t = sigmoid(U_F H_{t-1} + W_F X_t)$$\n",
    "\n",
    "<span style=\"color:red\">$\\rightarrow$ **\"기존의 정보를 얼마나 잊어버릴 것인가?\"**</span>\n",
    "\n",
    "**(2-1) 새로운입력(Input Candidate):** RNN 은닉층처럼 `새로운 정보 추출`\n",
    "\n",
    "- **tanh:** `RNN의 활성화 함수`로 relu로 대체될 수도 있음\n",
    "\n",
    "$$G_t = tanh(U_G H_{t-1} + W_G X_t)$$\n",
    "\n",
    "**(2-2) 입력게이트(Input Gate):** `새로운 정보의 가치`를 판단하여 `장기예측에 도움될지 추정`\n",
    "\n",
    "$$I_t = sigmoid(U_I H_{t-1} + W_I X_t)$$\n",
    "\n",
    "<span style=\"color:red\">$\\rightarrow$ **\"새로운 정보를 얼마나 기억할 것인가?\"**</span>\n",
    "\n",
    "**(3) 출력게이트(Output Gate):**  `1) 현재입력, 2) 과거단기, 3) 과거장기 정보를 반영`함과 동시에 `미래 시점의 예측값`으로 출력\n",
    "\n",
    "$$O_t = sigmoid(U_O H_{t-1} + W_O X_t) \\\\ H_t = O_t * tanh(C_t) \\\\ Y_t = f(H_t) = softmax(VH_t)$$\n",
    " \n",
    "<span style=\"color:red\">$\\rightarrow$ **\"`3) 과거 장기정보`($C_{t-1}$)는 필터링되고 현재의 새로운입력과 과거 단기정보가 추가되어 업데이트 되며($C_t$), `1) 현재입력, 2) 과거단기`와 결합되어 `새로운 단기 예측값`($H_t, Y_t$)을 출력\"**</span>\n",
    "\n",
    "- 입출력이 복잡해 보이지만 `수학/행렬적 연산은 심플`\n",
    "\n",
    "<center><img src='Image/Expert/LSTM_Summary.PNG' width='700'>(http://cs231n.stanford.edu/)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit\n",
    "\n",
    "> **\"`RNN의 단기기억문제 해결책`으로 고안되었고, Gate 구조가 적용된 LSTM에서 영감을 받아 `간결한 구조를 채택하여 계산상 효율적이고 유사한 성능`(Cho et al. 2014)\"**\n",
    ">\n",
    "> - LSTM이 Sequence 데이터 특화라면 `GRU는 자연어 데이터 특화 모델로 LSTM의 간소화 버전`\n",
    "> - LSTM은 Gate가 3개지만 `GRU는 Gate가 2개` (Reset Gate & Update Gate(Forget+Input 유사))\n",
    ">> - Convex 조합을 수행하는 두개의 벡터라고 볼 수 있음 (0 or 1)   \n",
    "> - Output Gate가 없는 LSTM이기에 메모리셀에 담기는 정보 양 증가\n",
    "> - LSTM의 $C_t$와 $H_t$가 Cell State를 대신하는 하나의 벡터 $H_t$로 통합\n",
    "> - GRU가 LSTM보다 학습할 가중치가 적은 이점\n",
    "> - 주제별로 LSTM과 GRU의 성능은 차이\n",
    ">\n",
    "> <center><img src='Image/Expert/DL_LSTM_GRU.png' width='700'>(https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Expert/DL_GRU_Architecture.png' width='350'></center>\n",
    "\n",
    "**(1) 초기화게이트(Reset Gate):** `과거의 데이터를 어느 정도의 비율로 제거`할지 결정\n",
    "\n",
    "$$R_t = sigmoid (U_R H_{t-1} + W_R X_t)$$\n",
    "\n",
    "**(2-1) 업데이트게이트(Update Gate):** `현재의 데이터를 얼마나 반영`할지 결정(`Input층과 유사`)\n",
    "\n",
    "$$U_t = sigmoid (U_U H_{t-1} + W_U X_t)$$\n",
    "\n",
    "**(2-2) 업데이트게이트(Update Gate):** `현재에 잊어버려야 할 데이터의 비율` 결정(`Forget층과 유사`)\n",
    "\n",
    "$$1 - U_t$$\n",
    "\n",
    "- Update Gate($U_t$)로 `LSTM의 Forget과 Input 게이트를 모두 제어`\n",
    "- 즉, $t-1$의 기억과 $t$의 `기억중 하나만 선택`\n",
    "\n",
    "**(3) 새로운입력(Input Candidate):** 과거를 그대로 사용하지 않고 `리셋 데이터`로 `장기예측에 도움될 새로운 정보 출력`   \n",
    "\n",
    "- $H_t$가 각 시점마다 출력되며 $H_{t-1}$의 `어느정도 비율을 출력할지 제어`하는 $R_t$\n",
    "\n",
    "$$h_t = tanh (U_h H_{t-1}*R_t + W_h X_t)$$\n",
    "\n",
    "**(4) 은닉층(Output) :** 출력층이 따로없지만 `Update와 Candidate 결합`하여 `미래 시점의 예측값으로 출력`\n",
    "\n",
    "$$H_t = (1 - U_t)*H_{t-1} + U_t*h_t$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Expert/CNN_RNN_MLP_SVM.PNG' width='800'></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## AutoEncoder -->"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1179.32px",
    "left": "21px",
    "top": "110.229px",
    "width": "281.52px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
